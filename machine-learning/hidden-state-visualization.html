<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hidden State Visualization of Llama and SmolLM Models | Ali's Blog </title> <meta name="author" content="Ali Faraji"> <meta name="description" content="Visualizing the hidden states of Llama and SmolLM language models to understand their token prediction capabilities."> <meta name="keywords" content="Ali Faraji, blog, machine learning, computer science."> <meta property="og:site_name" content="Ali's Blog"> <meta property="og:type" content="website"> <meta property="og:title" content="Ali's Blog | Hidden State Visualization of Llama and SmolLM Models"> <meta property="og:url" content="https://blog.faraji.info/machine-learning/hidden-state-visualization.html"> <meta property="og:description" content="Visualizing the hidden states of Llama and SmolLM language models to understand their token prediction capabilities."> <meta property="og:image" content="general/profile_pic.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Hidden State Visualization of Llama and SmolLM Models"> <meta name="twitter:description" content="Visualizing the hidden states of Llama and SmolLM language models to understand their token prediction capabilities."> <meta name="twitter:image" content="general/profile_pic.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Ali Faraji"
        },
        "url": "https://blog.faraji.info/machine-learning/hidden-state-visualization.html",
        "@type": "WebSite",
        "description": "Visualizing the hidden states of Llama and SmolLM language models to understand their token prediction capabilities.",
        "headline": "Hidden State Visualization of Llama and SmolLM Models",
        
        "sameAs": ["https://github.com/alifa98","https://www.linkedin.com/in/alifaraji98","https://orcid.org/0000-0002-2439-8493","https://scholar.google.com/citations?user=HUGFaVAAAAAJ","https://twitter.com/AliFaraji312230","https://faraji.info/"],
        
        "name": "Ali Faraji",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/general/favicon.png?5ecbbe85efeb8cb4bffba6fac83dd76e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://blog.faraji.info/machine-learning/hidden-state-visualization.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ali's Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/index.html">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/inspiration-shelf/">🤓 Inspiration Shelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/shower-thoughts/">🚿 Shower Thoughts </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">👋 About Me </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hidden State Visualization of Llama and SmolLM Models</h1> <p class="post-meta"> Created on November 26, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> LLMs   <i class="fa-solid fa-hashtag fa-sm"></i> Hidden States   <i class="fa-solid fa-hashtag fa-sm"></i> Visualization   <i class="fa-solid fa-hashtag fa-sm"></i> UMAP   <i class="fa-solid fa-hashtag fa-sm"></i> Llama   <i class="fa-solid fa-hashtag fa-sm"></i> SmolLM   ·   <i class="fa-solid fa-tag fa-sm"></i> Machine Learning   <i class="fa-solid fa-tag fa-sm"></i> LLMs </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I was playing around with hidden states of the language model as I was trying to implement an unlearning/manipulation method. So I thought it would be nice to visualize the hidden states of the model for two samples to show how they learn to predict the classification (the next token). I used the Llama and SmolLM models for this experiment.</p> <p>The dataset is 25 sentences where the next token would be Toronto, and 25 sentences where the next token should be Montreal. I used the Llama 3.2 and SmolLM versions 1 and 2 models to predict the next token, and then I visualized the hidden states of the models with the help of UMAP to reduce the dimensionality of the hidden states to 2D.</p> <p>This is an experiment that tells you why you should not use the hidden states of the model in any layer to embed your data or calculate the similarity between the data points. The hidden states are unpredictable and not interpretable. They are learned to predict the next token, and they are not learned to be used as embeddings.</p> <p>I have created two small datasets:</p> <ul> <li>One is 25 sentences that are complete and talk about Toronto, and 25 sentences that are complete and talk about Montreal.</li> <li>The second one is 25 sentences that are incomplete and talk about Toronto, and 25 sentences that are incomplete and talk about Montreal, where the immediate next token is the city name to be predicted.</li> </ul> <p>Also, I visualized the hidden state of the last token in each layer as we know that it is used to predict the next token, because the model is autoregressive and predicts the next token based on the hidden state of the last token, which contains all the information of the sentence so far.</p> <h2 id="incomplete-sentences">Incomplete Sentences</h2> <p>In these experiments, I used the incomplete sentences to predict the next token. The next token should be the city name, Toronto or Montreal. The model should predict the next token based on the hidden state of the last token in the sentence.</p> <h3 id="smollm-17b-for-incomplete-sentences">SmolLM 1.7B for incomplete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-1.7b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-1.7b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-1.7b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-1.7b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the SmolLM 1.7B model for the incomplete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="smollm-135m-for-incomplete-sentences">SmolLM 135M for incomplete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-135m-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-135m-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-135m-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm-135m.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the SmolLM 135M model for the incomplete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="smollm2-17b-for-incomplete-sentences">SmolLM2 1.7B for incomplete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm2-1.7b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm2-1.7b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm2-1.7b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis_huggingfacetb-smollm2-1.7b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the SmolLM2 1.7B model for the incomplete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="llama-32-1b-for-incomplete-sentences">Llama 3.2 1B for incomplete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-1b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-1b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-1b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-1b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the Llama 3.2 1B model for the incomplete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="llama-32-3b-for-incomplete-sentences">Llama 3.2 3B for incomplete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-3b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-3b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-3b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis_meta-llama-llama-3.2-3b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the Llama 3.2 3B model for the incomplete sentences dataset.</figcaption> </figure> </div> </div> <h2 id="complete-sentences">Complete Sentences</h2> <p>In these experiments, I used the complete sentences to predict the next token the latest token here is the punctuation mark period. I do not know what is the next token, I am just interested to see how the hidden states are learned to predict the next token based on the sentence.</p> <h3 id="smollm-17b-for-the-complete-sentences">SmolLM 1.7B for the complete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-1.7b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-1.7b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-1.7b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-1.7b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the SmolLM 1.7B model for the complete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="smollm-135m-for-the-complete-sentences">SmolLM 135M for the complete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-135m-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-135m-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-135m-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm-135m.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">cThe visualization of the hidden states of the SmolLM 135M model for the complete sentences dataset.c</figcaption> </figure> </div> </div> <h3 id="smollm2-17b-for-the-complete-sentences">SmolLM2 1.7B for the complete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm2-1.7b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm2-1.7b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm2-1.7b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis-c-s_huggingfacetb-smollm2-1.7b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the SmolLM2 1.7B model for the complete sentences dataset.</figcaption> </figure> </div> </div> <h3 id="llama-32-1b-for-the-complete-sentences">Llama 3.2 1B for the complete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-1b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-1b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-1b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-1b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">cThe visualization of the hidden states of the Llama 3.2 1B model for the complete sentences dataset.c</figcaption> </figure> </div> </div> <h3 id="llama-32-3b-for-the-complete-sentences">Llama 3.2 3B for the complete sentences</h3> <div class="row"> <div class="col-12 col-md-12 mx-auto d-block"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-3b-480.webp 480w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-3b-800.webp 800w,/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-3b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hidden_states_llms/hidden_state_vis-c-s_meta-llama-llama-3.2-3b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The visualization of the hidden states of the Llama 3.2 3B model for the complete sentences dataset.</figcaption> </figure> </div> </div> <h2 id="interpretation">Interpretation</h2> <p>The visualizations represent the 2D embeddings of the hidden states from multiple layers of different language models (Llama 3.2 and SmolLM variants) trained to predict the next token based on a sequence of tokens. The dimensionality reduction using UMAP reveals the patterns in the hidden state representations for sentences referring to “Toronto” (red points) and “Montreal” (blue points).</p> <h3 id="comparison-of-complete-vs-incomplete-sentences">Comparison of Complete vs. Incomplete Sentences</h3> <ul> <li>For incomplete sentences, the hidden states tend to form tighter clusters.</li> <li>For complete sentences, the clusters are less defined in the layers suggestion that the embeddings are just learned to predict the next token and they are not learned to be used as an embedding of city or the context of the sentence.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This experiment highlights the following key points:</p> <h3 id="hidden-states-are-model-specific-and-task-specific">Hidden States Are Model-Specific and Task-Specific</h3> <ul> <li>The visualizations clearly demonstrate that the hidden states are optimized to predict the next token rather than serve as a general-purpose embedding or similarity measure. Using them for tasks other than what they were trained for (e.g., semantic similarity or clustering) can yield unpredictable results.</li> </ul> <h3 id="layer-wise-variability">Layer-Wise Variability</h3> <ul> <li>The separation between classes (“Toronto” vs. “Montreal”) is not consistent across layers, indicating that different layers capture different aspects of the input sequence. This variability underscores the lack of interpretability of hidden states across layers.</li> </ul> <p>These results provide insights into the limitations of relying on hidden states and the importance of understanding model behavior at a deeper level.</p> <p>I will try to post more experiments on how we can manipulate the hidden states of the model to unlearn the data or to manipulate the data in a way that the model will not be able to predict the next token correctly.</p> <p>– Ali</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/machine-learning/icml-2025-top-papers.html">ICML 2025 top 50 Papers That Might Shape the Future of ML — According to Gemini</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/machine-learning/trajectory-foundation-model.html">I can speak Toronto: Pretraining a Trajectory Foundation Model</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/math/tool/bloom-taxonomy.html">Bloom's Taxonomy for Mathematics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/math/graph-theory/corona-product.html">Corona Product in Graph Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/llms/training-data-poisoning.html">Training data poisoning to get what you want in LLMs, A Question</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'alifa98/blog-new',
        'data-repo-id': 'R_kgDOPjkt-w',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOPjkt-84CvVaP',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Ali Faraji. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 16, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>